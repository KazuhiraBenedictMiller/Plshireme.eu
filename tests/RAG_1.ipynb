{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a91a175-785c-4dd8-b2e0-46b038e6c57b",
   "metadata": {},
   "source": [
    "# Super Simple End-to-End RAG implementation (no Pipeline just \"Ragging around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55627080-8ecb-4df9-bb3a-b8bafac52368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting API Keys and Env Variables\n",
    "\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'\n",
    "os.environ['GROQ_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19dfa26f-b185-4cb9-89b0-b4fc91d1c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Groq client\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "GroqClient = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c177e9-906b-42ac-9f8d-5bd9944c0323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first President of the United States was George Washington. He was inaugurated on April 30, 1789 and served two terms in office until March 4, 1797.\n"
     ]
    }
   ],
   "source": [
    "#Testing the Groq Client\n",
    "\n",
    "chat_completion = GroqClient.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who was the first U.S.A. President?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35985a-b45c-4a5a-a31c-ae4f2e219bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring Groq API\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an History Teacher Specialized in U.S. American history\" #. You always return your answer in JSON format.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who was the first U.S.A. President?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    "    #Temperature controls randomness: As the temperature approaches zero, the model will become deterministic and repetitive, Values from 0 to 2 - can be float.\n",
    "    #If you adjust the temperature to 0.5, the model will generate text that is more predictable and less creative than if you set the temperature to 1.0.\n",
    "    temperature=1,\n",
    "    #Max output Tokens\n",
    "    max_tokens=1024,\n",
    "    #Top_P controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered, Values from 0 to 1 - can be float.\n",
    "    #If you set top p to 0.9, the model will only consider the most likely words that make up 90% of the probability mass.\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    #stream=True,\n",
    "    #response_format={\"type\": \"json_object\"},\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "#If Stream set to True\n",
    "#for chunk in chat_completion:\n",
    "#    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "#If Stream set to False and response format not JSON\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "#If using JSON output, ensure Stream is set to False and System prompt contains the word JSON contextualized such as \"You always return your answer in JSON format.\".\n",
    "#print(chat_completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4d7940-17c1-4835-af36-2d19d3a993f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in ./LLMvenv/lib/python3.11/site-packages (0.1.4)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in ./LLMvenv/lib/python3.11/site-packages (from langchain-groq) (0.8.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.45 in ./LLMvenv/lib/python3.11/site-packages (from langchain-groq) (0.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.7.1)\n",
      "Requirement already satisfied: sniffio in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./LLMvenv/lib/python3.11/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./LLMvenv/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./LLMvenv/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./LLMvenv/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (0.1.62)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./LLMvenv/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./LLMvenv/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (8.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./LLMvenv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
      "Requirement already satisfied: certifi in ./LLMvenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./LLMvenv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./LLMvenv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./LLMvenv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain-groq) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./LLMvenv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./LLMvenv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (2.32.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./LLMvenv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in ./LLMvenv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./LLMvenv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./LLMvenv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1c51ee-24e4-4881-86ba-608dcf1f5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/Desktop/WideconsPrototypes/LLM/LLMvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/nomad/Desktop/WideconsPrototypes/LLM/LLMvenv/lib/python3.11/site-packages/langchain_groq/chat_models.py:147: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex problem or task into smaller, more manageable subtasks or steps. This is often done to make the task more feasible and to identify the necessary steps required to achieve the desired outcome.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Super Simple End-to-End Implementation\n",
    "\n",
    "#Importing Libraries\n",
    "\n",
    "import bs4\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=1, top_p=1, max_tokens=1024)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba804ea-fee4-4d26-95cf-79646edbd15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\nAnswer the question based only on the following context: {context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prompt from LangchainHub\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140d44e3-1ab6-44ab-b68d-57e35e5263f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\nAnswer the question based only on the following context: {context}\\n\\nQuestion: {question}\\n'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prompt Building Example\n",
    "\n",
    "# Prompt\n",
    "template = '''\n",
    "Answer the question based only on the following context: {context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b65cd03c-09f0-41c2-84d9-eb848f2b0543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what's the difference between llamaindex and langchain\"\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4efbbc68-1534-4155-b4bf-8bc48801fd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "Cosine Similarity: 0.5595268901544017\n"
     ]
    }
   ],
   "source": [
    "#Embedding the Question and computing Cosine Similarity\n",
    "\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "query_result = embedding.embed_query(question)\n",
    "document_result = embedding.embed_query(document)\n",
    "print(len(query_result))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
